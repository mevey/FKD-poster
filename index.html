<head>
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Roboto" rel="stylesheet">
    <style>
        body {font-family: 'Roboto', sans-serif;padding:20px; -webkit-print-color-adjust:exact;}
        .container{ width: 100%; padding:40px; background-color:#fff; padding:15px; background-image:url('https://st.depositphotos.com/1032463/1373/i/950/depositphotos_13732950-stock-photo-background-of-old-vintage-newspapers.jpg')}
        h1, h4 {font-family: 'Open Sans', sans-serif; color:brown;text-transform:uppercase;}
        h1 {padding-bottom:70px;text-align:center;font-weight:500;color:brown !important; font-size:50px;}
        h4 {color:#fff !important;background-color:brown !important;padding:5px 10px;margin-bottom:20px;font-size:30px;}
        table {font-family: 'Roboto', sans-serif !important;}
        th {font-family: 'Open Sans', sans-serif; font-weight:600 !important;}
        td {font-family: 'Roboto', sans-serif;}
        p ,li,b {line-height:28px; font-size:18px;}
        .contact {display:inline-block;padding-right:5px;}
        .row {background-color:#fff !important; }
        @media screen, print {
            body {
                -webkit-print-color-adjust: exact;
            }
            .bigger {
                background-image:url('https://st.depositphotos.com/1032463/1373/i/950/depositphotos_13732950-stock-photo-background-of-old-vintage-newspapers.jpg') !important;
            }
            h4 {
                background-color:brown !important;
                -webkit-print-color-adjust: exact;
                color:#fff !important;
            }
            h1 {
                color:brown !important;
            }
            .row {background-color:#fff !important; }
            .container{padding:40px;background-image:url('https://st.depositphotos.com/1032463/1373/i/950/depositphotos_13732950-stock-photo-background-of-old-vintage-newspapers.jpg') !important;}
        }


    </style>
</head>

<body>
<div class="container">
    <div class="row">
        <div class="col-md-12">
            <h1>FAKE NEWS DETECTION USING STYLISTIC FEATURES AND GOOGLE NEWS WORD EMBEDDINGS</h1>
        </div>
        </div>
        <div class="row">
            <div class="col-md-4">
                <h4>SYNOPSIS</h4>
                <p>Fake news is a hot topic in today’s society. The average voter had been exposed to at least one fake story in the months before the US 2016 election, and that about half had believed the deceptive news.
                    Substantial progress has been made in this direction using a variety of approaches, but a lack of quality training data has limited research on text-based fake news detection.
                    <br>
                    In this paper, we analyze text-based features for fake news detection using one of the largest and most comprehensive fake news datasets available: The Buzzfeed-Webis Fake News Corpus 2016. We will apply a logistic regression model on the text and metadata related to each article to test which features are useful for fake news detection. Additionally, we pre-trained a CNN fake news detector using the Google News word embeddings, and compared results with our logistic regression output.</p>
                <br>
                <h4>DATA</h4>
                <p>
                    The Buzzfeed-Webis Fake News Corpus 2016 (Silverman, Strapagiel, Shaban, Hall, Singer-Vine, 2016) contains 1,627 news articles that were manually fact-checked by labelled as follows.
                    <table class="table table-bordered">
                        <thead>
                            <th>Label</th>
                            <th class="text-center">Percentage</th>
                        </thead>
                        <tbody>
                            <tr>
                                <td> Mostly true</td>
                                <td class="text-center">77.6%</td>
                            </tr>
                            <tr>
                                <td>Mixture of true and false</td>
                                <td class="text-center">13.0%</td>
                            </tr>
                            <tr>
                                <td>Mostly False</td>
                                <td class="text-center">5.3%</td>
                            </tr>
                            <tr>
                                <td>No factual content</td>
                                <td class="text-center">3.9%</td>
                            </tr>
                        </tbody>
                    </table>
                    <!--The data is from 9 publishers. 3 left-wing, 3 right-wing and 3 main stream. The original data contained the main body of text, author information, orientation, number of links and quotes in the text.  We generated features from this content examples of which are:-->
                </p>
                <h4>MODELS</h4>
                <p>
                    1. Hierarchical CNN (three convolutional and max pooling layers, the first two of which are followed by dropout layers). Grid search was used to select the best performing configuration of hyperparameters for the CNN.<br>
                    <br>
                    2. For the logistic regression model we used two representations of the data; TF-IDF and word embeddings. StratifiedKFold search was also used to select the regularization strength.
                </p>
            </div>
            <div class="col-md-3">
                <div class="text-center">
                    <img src="CNN.png"/>
                    <br clear="all">
                    <small><i> Fig 1. CNN architecture </i></small>
                 </div>
            </div>
            <div class="col-md-5">
                <h4>EVALUATION</h4>
                <table class="table table-bordered">
                    <thead>
                        <th>Model</th>
                        <th class="text-center">Accuracy</th>
                        <th class="text-center">Avg. Precision</th>
                        <th class="text-center">Avg. Recall</th>
                        <th class="text-center">Avg. F1</th>
                    </thead>
                    <tbody>
                        <tr>
                            <td> CNN </td>
                            <td class="text-center">81.3%</td>
                            <td class="text-center">0.66</td>
                            <td class="text-center">0.81</td>
                            <td class="text-center">0.73</td>
                        </tr>
                        <tr>
                            <td>Logistic Regression(embeddings)</td>
                            <td class="text-center">81.3%</td>
                            <td class="text-center">0.66</td>
                            <td class="text-center">0.81</td>
                            <td class="text-center">0.73</td>
                        </tr>
                        <tr>
                            <td>Logistic Regression(TF-IDF)</td>
                            <td class="text-center">81.6%</td>
                            <td class="text-center">0.69</td>
                            <td class="text-center">0.82</td>
                            <td class="text-center">0.74</td>
                        </tr>
                    </tbody>
                </table>
                <br>
                <h4>ANALYSIS</h4>
                <p>
                    <b>Are deception detection textual traits still useful in prediction Fake News?</b>
                    <ul>
                        <li>Our analysis does not find textual features to improve classification accuracy in fake news detection. </li>
                        <li>Fake news is similar to real news in most deception features - furthermore, the sparsity of fake news impedes classification </li>
                    </ul>
                    <b>What representations of fake news text are most effective?</b>
                    <ul>
                        <li>We found tf-idf representations of article text achieve marginally better results than the Google News pre-trained word embeddings in our logistic regression model. </li>
                        <li>This is a surprising result because pre-trained embeddings generally outperform tf-idf representations made over small datasets. Furthermore, the Google News embeddings were trained in a very similar domain (Google News stories) and thus would be expected to be effective in this case. </li>
                        <li>Articles were represented as the element-wise average of the embeddings of their constituent words - this process may have removed some of the representative power of the embeddings.</li>
                        <li>In the case of the CNN model, pre-trained embeddings were necessary to achieve meaningful accuracy - 1,600 relatively short articles are too few to effectively train word embeddings.</li>
                    </ul>
                    <b>How do manually-created features compare to a neural model for fake news classification?</b>
                    <ul>
                        <li>A logistic regression model outperforms a CNN in the case of our dataset, although neither is effective at discriminating fake news from real articles. This is likely due in part to the class imbalance and size of our dataset. </li>
                        <li>Because “mostly true” stories make up 78% of the dataset, the CNN learns to predict the majority class in almost all cases, regardless of loss function or hyperparameter choices. Logistic regression only outperforms the CNN in identifying non factual content.</li>
                        <li>Strategies for dealing with class imbalance (including class weighting) are not as effective when applied to such a small dataset - weighting could keep the model from only choosing the majority class, but tremendously reduces accuracy</li>
                        <li>Our findings suggest applying a CNN model to such a small and imbalanced dataset is not feasible - neural models would likely be better suited to a larger dataset</li>
                    </ul>
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <h4 class="contact">CONTACT</h4>
                <span>Tim Burke <a>timothy.burke@berkeley.edu</a>,&nbsp;</span> Eve Mwangi <a>evelyn_mwangi@berkeley.edu</a></span>
            </div>
        </div>
    </div>
</body>

