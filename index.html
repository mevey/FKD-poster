<head>
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Roboto" rel="stylesheet">
    <style>
        body {font-family: 'Roboto', sans-serif;padding:20px; background-image:url('https://st.depositphotos.com/1032463/1373/i/950/depositphotos_13732950-stock-photo-background-of-old-vintage-newspapers.jpg')}
        .container{ width: 95%; background-color:#fff; padding:15px;}
        h1, h4 {font-family: 'Open Sans', sans-serif; color:brown;text-transform:uppercase;}
        h1 {padding-bottom:10px;text-align:center;font-weight:500;}
        table {font-family: 'Roboto', sans-serif !important;}
        th {font-family: 'Open Sans', sans-serif; font-weight:600 !important;}
        td {font-family: 'Roboto', sans-serif;}
        p ,li {line-height:24px;}

    </style>
</head>

<body>
<div class="container">
    <div class="row">
        <div class="col-md-12">
            <h1>FAKE NEWS DETECTION USING STYLISTIC FEATURES AND GOOGLE NEWS WORD EMBEDDINGS</h1>
        </div>
        </div>
        <div class="row">
            <div class="col-md-4">
                <h4>SYNOPSIS</h4>
                <p>Fake news is a hot topic in today’s society. Allcott and Gentkow found that the average voter had been exposed to at least one fake story in the months before the US 2016 election, and that about half had believed the deceptive news. Substantial progress has been made in this direction using a variety of approaches, but a lack of quality training data has limited research on text-based fake news detection. In this paper, we analyze text-based features for fake news detection using one of the largest and most comprehensive fake news datasets available: The Buzzfeed-Webis Fake News Corpus 2016. We will apply a logistic regression model on the text and metadata related to each article to test which features are useful for fake news detection. Additionally, we pre-trained a CNN fake news detector using the Google News word embeddings, and compared results with our logistic regression output.</p>
                <br>
                <h4>DATA</h4>
                <p>
                    The Buzzfeed-Webis Fake News Corpus 2016 (Silverman, Strapagiel, Shaban, Hall, Singer-Vine, 2016) contains 1,627 news articles that were manually fact-checked by labelled as follows.
                    <table class="table">
                        <thead>
                            <th>Label</th>
                            <th>Percentage</th>
                        </thead>
                        <tbody>
                            <tr>
                                <td> Mostly true</td>
                                <td>77.6%</td>
                            </tr>
                            <tr>
                                <td>Mixture of true and false</td>
                                <td>13.0%</td>
                            </tr>
                            <tr>
                                <td>Mostly False</td>
                                <td>5.3%</td>
                            </tr>
                            <tr>
                                <td>No factual content</td>
                                <td>3.9%</td>
                            </tr>
                        </tbody>
                    </table>
                    The data is from 9 publishers. 3 left-wing, 3 right-wing and 3 main stream. The original data contained the main body of text, author information, orientation, number of links and quotes in the text.  We generated features from this content examples of which are:
                </p>
            </div>
            <div class="col-md-4">
                <h4>MODEL</h4>
                <p>
                    Manually designed text features and black box vs feature engineering<br>
                    -Hierarchical CNN (If we can make/find an image, it could show the three convolutional and max pooling layers, the first two of which are followed by dropout layers).<br>
                    -Mention the hierarchical CNN was cross-validated using grid search.<br>
                </p>
                <br>
                <i>Fake model</i>
                <img src="https://explosion.ai/blog/img/deep-learning-formula-nlp_example2.svg">

            </div>
            <div class="col-md-4">
                <h4>ANALYSIS</h4>
                <p>
                    <b>Are deception detection textual traits still useful in prediction Fake News?</b>
                    <ul>
                        <li>Our analysis does not find textual features to improve classification accuracy in fake news detection.</li>
                        <li>The relative sparsity of fake news articles among the dataset is one issue - it is difficult</li>
                    </ul>
                    <b>What representations of fake news text are most effective?</b>
                    <ul>
                        <li>We found tf-idf representations of article text to be the most effective for our logistic regression model, achieving marginally better results than the Google News pre-trained word embeddings.</li>
                        <li>This is a surprising result because pre-trained embeddings generally outperform tf-idf representations made over small datasets. Furthermore, the Google News embeddings were trained in a very similar domain (Google News stories) and thus would be expected to be effective in this case.</li>
                        <li>Articles were represented as the element-wise average of the embeddings of their constituent words - this process may have removed some of the representative power of the embeddings.</li>
                        <li>In the case of the CNN model, representing words are necessary to achieve any meaningful accuracy - 1,600 relatively short articles are too few to effectively train word embeddings.</li>
                    </ul>
                    <b>How do manually-created features compare to a neural model for fake news classification?</b>
                    <ul>
                        <li>A logistic regression model outperforms a CNN in the case of our dataset. This is likely due in part to the class imbalance and size of our dataset.</li>
                        <li>Because “mostly true” stories make up 78% of the dataset, the CNN learns to predict the majority class in almost all cases, regardless of loss function or hyperparameter choices.</li>
                        <li>Strategies for dealing with imbalanced data (including class weighting) are not as effective when applied to such a small dataset - weighting could keep the model from only choosing the majority class, but result in horrendous accuracy</li>
                        <li>Applying neural models to a larger labeled corpus may well be effective, but in the case of our dataset we find that logistic regression shows the strongest performance.</li>
                    </ul>
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <h4>CONTACT</h4>
                <span>Tim Burke <a>timothy.burke@berkeley.edu</a>, </span> Eve Mwangi <a>evelyn_mwangi@berkeley.edu</a></span>
            </div>
        </div>
    </div>
</body>

